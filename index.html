<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>Show Demos</title>
    <style>
      table {
        border-collapse: collapse;
        width: 50%;
      }

      th, td {
        border: 1px solid black;
        padding: 8px;
        text-align: left;
      }

      th {
        background-color: #f2f2f2;
      }
    </style>
  </head>
  <body>
    <h1>景观类视频生成</h1>
    <img src="landscape.gif" alt="景观类视频生成示例" />
    <h1>人像类视频生成</h1>
    <img src="man.gif" alt="人像类视频生成示例" />
    <h1>各公司方法与Project汇总</h1>
    <p>在上面我们摘取了各家公司Demo的很小一部分结果展示，大家可以通过点击如下表格的Project链接对比更多示例效果</p>
    <table>
        <tr>
            <th>公司</th>
            <td>方法</td>
            <td>Project</td>
        </tr>
        <tr>
            <th>Google</th>
            <td>Video Diffusion Models</td>
            <td><a href="https://video-diffusion.github.io">https://video-diffusion.github.io</a></td>
        </tr>
        <tr>
            <th>Meta</th>
            <td>Make-A-Video</td>
            <td><a href="https://makeavideo.studio">https://makeavideo.studio</a></td>
        </tr>
        <tr>
            <th>ByteDance</th>
            <td>MAGIC VIDEO</td>
            <td><a href="https://magicvideo.github.io">https://magicvideo.github.io</a></td>
        </tr>
        <tr>
            <th>Tencent</th>
            <td>Tune-A-Video</td>
            <td><a href="https://tuneavideo.github.io">https://tuneavideo.github.io</a></td>
        </tr>
        <tr>
            <th>Runway</th>
            <td>Gen2</td>
            <td><a href="https://research.runwayml.com/gen2">https://research.runwayml.com/gen2</a></td>
        </tr>
        <tr>
            <th>Ali</th>
            <td>VideoFusion</td>
            <td><a href="https://www.modelscope.cn/models/damo/cv_diffusion_text-to-video-synthesis/summary">https://www.modelscope.cn/models/damo/cv_diffusion_text-to-video-synthesis/summary</a></td>
        </tr>
        <tr>
            <th>NVIDIA</th>
            <td>Align your latent</td>
            <td><a href="https://research.nvidia.com/labs/toronto-ai/VideoLDM">https://research.nvidia.com/labs/toronto-ai/VideoLDM</a></td>
        </tr>
        <tr>
            <th>Microsoft</th>
            <td>NUWA-XL</td>
            <td><a href="https://msra-nuwa-dev.azurewebsites.net">https://msra-nuwa-dev.azurewebsites.net</a></td>
        </tr>
        <tr>
            <th>Meta</th>
            <td>Latent-shift</td>
            <td><a href="https://latent-shift.github.io">https://latent-shift.github.io</a></td>
        </tr>
        <tr>
            <th>Nivida</th>
            <td>Pyoco</td>
            <td><a href="https://research.nvidia.com/labs/dir/pyoco">https://research.nvidia.com/labs/dir/pyoco</a></td>
        </tr>
        <tr>
            <th>Sensetime</th>
            <td>Gen-L-Video</td>
            <td><a href="https://g-u-n.github.io/projects/gen-long-video/index.html">https://g-u-n.github.io/projects/gen-long-video/index.html</a></td>
        </tr>
        <tr>
            <th>智源</th>
            <td>CogVideo</td>
            <td><a href="https://models.aminer.cn/cogvideo">https://models.aminer.cn/cogvideo</a></td>
        </tr>
    </table>
    
  </body>
</html>


